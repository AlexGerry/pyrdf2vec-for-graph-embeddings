{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"walks_extraction.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyOLRswTyr/Xm2ABLnBvigrF"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"KQc_abMw4f0c","executionInfo":{"status":"ok","timestamp":1644411043429,"user_tz":-60,"elapsed":14475,"user":{"displayName":"Alessandro Gherardi","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"12460417813153369323"}}},"outputs":[],"source":["%%capture\n","!pip uninstall -y python-louvain community\n","!pip install aiohttp nest-asyncio rdflib python-louvain\n","!git clone https://github.com/AlexGerry/pyrdf2vec-for-graph-embeddings\n","!pip install ./pyRDF2Vec --use-feature=in-tree-build"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eSsawUdN2eHE"},"outputs":[],"source":["from pyrdf2vec import RDF2VecTransformer\n","from pyrdf2vec.graphs import KG\n","from pyrdf2vec import walkers, samplers\n","from cachetools import TTLCache"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3hO6ts6X2VaA"},"outputs":[],"source":["#set start entity for random walks\n","entities_dbpedia = [\n","   \"http://dbpedia.org/resource/Italy\"\n","]\n","\n","entities_wikidata = [\n","  'http://www.wikidata.org/entity/Q38' # Italy \n","]"]},{"cell_type":"code","source":["# Define our knowledge graph (here: DBPedia SPARQL endpoint).\n","knowledge_graph_dbpedia= KG(\n","  \"https://dbpedia.org/sparql\",\n","  literals=[\n","            [\n","             \"http://dbpedia.org/ontology/wikiPageWikiLink\"\n","             ]\n","            ],\n","  mul_req=True,\n","  cache=TTLCache(2048, 2400)\n",")\n","\n","# Define our knowledge graph for wikidata (here: Wikidata SPARQL endpoint).\n","knowledge_graph_wikidata = KG(\n","  \"https://query.wikidata.org/sparql\",\n","  query_string=\"query\",\n","  skip_predicates={},\n","  literals=[\n","            [\n","             \"http://www.w3.org/2004/02/skos/core#prefLabel\"]\n","            ],\n","  mul_req=True\n",")"],"metadata":{"id":"7Bmd2KpEgPBN"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9kb0ls04pXXp"},"outputs":[],"source":["# With with_reverse=True random walk starts from the provided entity (entity -> ... -> ... *max_depth*)\n","# Then for each walk starts another random walk (each with max_depth=... and max_walks=...) but backwords (*max_depth* ... -> ... -> entity)\n","# So max_walks*max_walks walks (*max_depth* ... -> ... -> entity -> ... -> ... *max_depth*) are produced \n","\n","walker = walkers.RandomWalker(max_depth=3, max_walks=10, with_reverse=True, md5_bytes=None)\n","\n","# Create our transformer, setting the embedding & walking strategy.\n","transformer = RDF2VecTransformer(\n","  walkers=[walker],\n","  verbose=1\n",")\n","#transformer and walker are the same for Dbpedia and wikidata"]},{"cell_type":"code","source":["# extraction of random walks for dbpedia\n","walks_dbpedia = transformer.get_walks(knowledge_graph_dbpedia, entities_dbpedia)"],"metadata":{"id":"RMr8leA2gwrK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ectraction of random walks for wikidata\n","walks_wikidata = transformer.get_walks(knowledge_graph_wikidata, entities_wikidata)"],"metadata":{"id":"mZ3C3N17g8ld"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#save dbpedia walks in a text file, not necessary, we already put in git repository our walks\n","resource = [[[i.replace(\"http://dbpedia.org/resource/\", \"\") for i in j if i.startswith(\"http://dbpedia.org/resource/\")] for j in k] for k in walks_dbpedia]\n","with open('dbpedia_walks_final.txt', 'wt') as f:\n","  for w in resource[0]:\n","    f.write(' '.join(w) + '\\n')"],"metadata":{"id":"FQ1gWeBbeYP1"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cTGf0VxV6fga"},"outputs":[],"source":["#save wikidata walks in a text file, not necessary, we already put in git repository our walks\n","resource = [[[i.replace(\"http://www.wikidata.org/entity/\", \"\") for i in j if i.startswith(\"http://www.wikidata.org/entity/\")] for j in k] for k in walks_wikidata]\n","with open('wikidata_walks_final.txt', 'wt') as f:\n","  for w in resource[0]:\n","    f.write(' '.join(w) + '\\n')"]}]}